{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1920,f_auto/Dog-main_gdcdzd.jpg)\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "The aim of the project is to identify a breed of dog if a photo is given as input. If the photo contains a human face (or alien face), then the application will return the breed of dog that most resembles this person.\n",
    "\n",
    "In this project I have used Convolutional Neural Networks (CNNs)! A pipeline is built to process real-world, user-supplied images. Given an image of a dog, the algorithm will identify an estimate of the canine’s breed. If supplied an image of a human, the code will identify the resembling dog breed.\n",
    "\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Building a model to classify between 133 different breeds of dogs and identify them\n",
    "\n",
    "**The Road Ahead**\n",
    "\n",
    "I break the notebook into separate steps to make the steps clear. The following is the steps that I followed during the project building time.\n",
    "\n",
    "1. Import Datasets\n",
    "2. Detect Humans\n",
    "3. Detect Dogs\n",
    "4. Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "5. Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "6. Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "7. Write your Algorithm\n",
    "8. Test Your Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Import Datasets and necessary libraries - The datasets is provided by Udacity.**\n",
    "\n",
    "In the code cell below, I imported the dataset of dog images. I populated a few variables through the use of the load_files function from the scikit-learn library:\n",
    "\n",
    "* train_files, validation_files, test_files - numpy arrays containing file paths to images\n",
    "* train_targets, validation_targets, test_targets - numpy arrays containing onehot-encoded classification labels\n",
    "* dog_names - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.datasets import load_files    \n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions \n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator                  \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    from keras.utils import np_utils\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Data Directories\n",
    "basedir = Path(os.getcwd())\n",
    "data_dir = Path(os.path.join(basedir,'Data'))\n",
    "train_dir = Path(os.path.join(basedir,'Data/dogImages/train'))\n",
    "valid_dir = Path(os.path.join(basedir,'Data/dogImages/valid'))\n",
    "test_dir = Path(os.path.join(basedir,'Data/dogImages/test'))\n",
    "algo_test_dir = Path(os.path.join(basedir,'test_images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load train, test, and validation datasets\n",
    "def get_datasets():\n",
    "    train_files, train_targets = load_dataset(train_dir)\n",
    "    valid_files, valid_targets = load_dataset(valid_dir)\n",
    "    test_files, test_targets = load_dataset(test_dir)\n",
    "    return train_files, train_targets, valid_files, valid_targets, test_files, test_targets\n",
    "\n",
    "train_files, train_targets, valid_files, valid_targets, test_files, test_targets = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"Data/dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Human Dataset**\n",
    "\n",
    "The follwoing cell imports dataset of human images, where the file paths are stored in the numpy array human_files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(32)\n",
    "#Assigning human dataset directory\n",
    "human_dir = os.path.join(data_dir, 'lfw/*/*')\n",
    "\n",
    "#Load names from, human dataset directory\n",
    "human_files = np.array(glob(human_dir))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "#Print ststistics about the dataset\n",
    "print('There are %d total human images.' %len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Detect Humans**\n",
    "\n",
    "I have used OpenCV's implementation of <a href=\"https://docs.opencv.org/trunk/db/d28/tutorial_cascade_classifier.html\"> Haar feature-based cascade classifiers </a>to detect human faces in images. OpenCV provides many pre-trained face detectors, stored as XML files on <a href=\"https://github.com/opencv/opencv/tree/master/data/haarcascades\">Github</a>. I have downloaded one of these detectors and stored it in the haarcascades directory.\n",
    "\n",
    "In the next code cell, I demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('./Data/haarcascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "\n",
    "num_human_faces_human_files = 0\n",
    "num_human_faces_dog_files = 0\n",
    "for i in  tqdm(range(0,100)):\n",
    "    num_human_faces_human_files += face_detector(human_files_short[i])\n",
    "    num_human_faces_dog_files += face_detector(dog_files_short[i])    \n",
    "    \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "print('% of human faces detected in human files {:2.2%}\\n % of human faces detected in dog files {:2.2%}'\\\n",
    "      .format(num_human_faces_human_files/100, num_human_faces_dog_files/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Detect Dogs**\n",
    "\n",
    "In this section, I use a pre-trained <a href='http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006'>ResNet-50</a> model to detect dogs in images. My first line of code downloads the ResNet-50 model, along with weights that have been trained on <a href='http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006'>ImageNet</a>, a very large, very popular dataset used for image classification and other vision tasks. ImageNet contains over 10 million URLs, each linking to an image containing an object from one of <a href='https://github.com/ravi-gopalan/dog_breed_classifier_udacity/blob/master/dog_breed_classifier.ipynb'>1000 categories</a>. Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NASNetmobile model\n",
    "NASNetmobile = tf.keras.applications.NASNetMobile(input_shape=(224, 224, 3),\n",
    "                                                    include_top=True, \n",
    "                                                    weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASNetmobile.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-process the Data**\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n",
    "\n",
    "The path_to_tensor function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN. The function first loads the image and resizes it to a square image that is 224×224 pixels. Next, the image is converted to an array, which is then resized to a 4D tensor. In this case, since we are working with color images, each image has three channels. Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "The paths_to_tensor function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape\n",
    "\n",
    "$$\n",
    "(\\text{nbsamples},224,224,3).\n",
    "$$\n",
    "Here, nb_samples is the number of samples, or number of images, in the supplied array of image paths. It is best to think of nb_samples as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions with ResNet-50 Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing. First, the RGB image is converted to BGR by reordering the channels. All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as [103.939,116.779,123.68] and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image. This is implemented in the imported function preprocess_input.\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions. This is accomplished with the predict method, which returns an array whose 𝑖 -th entry is the model's predicted probability that the image belongs to the 𝑖 -th ImageNet category. This is implemented in the ResNet50_predict_labels function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NASNetmobile_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(NASNetmobile.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a Dog Detector**\n",
    "\n",
    "While looking at the dictionary, you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from 'Chihuahua' to 'Mexican hairless'. Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the ResNet50_predict_labels function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the dog_detector function below, which returns True if a dog is detected in an image (and False if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = NASNetmobile_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_detected = 0\n",
    "for im in human_files_short:\n",
    "    dog_detected += dog_detector(im)\n",
    "    \n",
    "print('% of images in human_files with a dog detected: {:2.2%}'.format(dog_detected/len(human_files_short)))\n",
    "\n",
    "dog_detected = 0\n",
    "for im in dog_files_short:\n",
    "    dog_detected += dog_detector(im)\n",
    "    \n",
    "print('% of images in dog_files with a dog detected: {:2.2%}'.format(dog_detected/len(dog_files_short)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6680/6680 [00:44<00:00, 150.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 835/835 [00:05<00:00, 162.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 836/836 [00:05<00:00, 163.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "import timeit\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "\n",
    "model_tensor_creation_time_start = timeit.default_timer()\n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255\n",
    "\n",
    "model_tensor_creation_time_stop = timeit.default_timer()\n",
    "model_tensor_creation_time = model_tensor_creation_time_stop - model_tensor_creation_time_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images. In this step, you will create a CNN that classifies dog breeds. You must create your CNN from scratch (so, you can't use transfer learning yet!), and you must attain a test accuracy of at least 1%. In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers! More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process. Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train.\n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging. To see why, consider that even a human would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.\n",
    "\n",
    "Brittany | Welsh Springer Spaniel <img src=\"https://camo.githubusercontent.com/4b3524acdcb73ccb2014d90fdb606af2c81d92d4/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f632f63662f4d6f6e7479506f7274726169742e6a70672f32343070782d4d6f6e7479506f7274726169742e6a7067\" alt=\"Brittany\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cf/MontyPortrait.jpg/240px-MontyPortrait.jpg\">\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/c014aa15970a8dbd41840f021304b7d48baa0d12/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f352f35652f57656c73685f537072696e6765725f5370616e69656c5f446f672e6a70672f31323170782d57656c73685f537072696e6765725f5370616e69656c5f446f672e6a7067\" alt=\"Welsh Springer Spaniel\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Welsh_Springer_Spaniel_Dog.jpg/121px-Welsh_Springer_Spaniel_Dog.jpg\">\n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel <img src=\"https://camo.githubusercontent.com/7c6c0d6db30a1efdd5ea0731d23a4e9e13c98154/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f312f31642f4375726c795f436f617465645f5265747269657665725f2d5f3030312d322d322e6a70672f31383070782d4375726c795f436f617465645f5265747269657665725f2d5f3030312d322d322e6a7067\" alt=\"Curly_Coated_Retriever\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1d/Curly_Coated_Retriever_-_001-2-2.jpg/180px-Curly_Coated_Retriever_-_001-2-2.jpg\">\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/a8a9f1ad70291b64a02da21a9c848ba91fe6436b/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f332f33662f416d65726963616e5f57617465725f5370616e69656c5f507570706965735f30322e6a70672f31343870782d416d65726963616e5f57617465725f5370616e69656c5f507570706965735f30322e6a7067\" alt=\"American Water Spaniel\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3f/American_Water_Spaniel_Puppies_02.jpg/148px-American_Water_Spaniel_Puppies_02.jpg\">\n",
    "\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black. Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.\n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/f00781af2fd17663d3408b88b1121685d4275c78/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f332f33382f59454c4c4f575f4c41425241444f525f5245545249455645522e6a70672f31323870782d59454c4c4f575f4c41425241444f525f5245545249455645522e6a7067\" alt=\"Yellow Labrador\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/YELLOW_LABRADOR_RETRIEVER.jpg/128px-YELLOW_LABRADOR_RETRIEVER.jpg\">\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/4b4c6f8d37380040e3a5c513985c6978eb729e41/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f342f34362f43686f636f6c6174655f4c61627261646f725f253238363832393836303330332532392e6a70672f31363870782d43686f636f6c6174655f4c61627261646f725f253238363832393836303330332532392e6a7067\" alt=\"Chocolate Labrador\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Chocolate_Labrador_%286829860303%29.jpg/168px-Chocolate_Labrador_%286829860303%29.jpg\">\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/eccedae572a8f4166ef3a756006804d6e4317346/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f7468756d622f342f34612f426c61636b5f4c61627261646f725f5265747269657665725f706f7274726169742e6a70672f31363870782d426c61636b5f4c61627261646f725f5265747269657665725f706f7274726169742e6a7067\" alt=\"Black Labrador\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4a/Black_Labrador_Retriever_portrait.jpg/168px-Black_Labrador_Retriever_portrait.jpg\">\n",
    "\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.\n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning. Experiment with many different architectures, and trust your intuition. And, of course, have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Define your architecture.\n",
    "\n",
    "def conv_layer(x, num_filters, ks, pad='valid', strides=1, scope='conv', batch_norm=False, **opts):\n",
    "    # x = tf.keras.layers.ZeroPadding2D(padding=pad, name=scope + '/pad')(x)\n",
    "    #x = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=ks, padding=pad, name=scope + '/conv', **opts)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(name=scope + '/bn')(x) if batch_norm else x\n",
    "    x = tf.keras.layers.ReLU(name=scope + '/relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(x, filters, ks=3, dropout_rate=None, scope='block', **opts):\n",
    "    block_name = scope\n",
    "    if dropout_rate:\n",
    "        x = tf.keras.layers.SpatialDropout2D(dropout_rate, name=block_name + '/dp')(x)\n",
    "    x = conv_layer(x, filters[0], ks, 'same', scope=block_name + '/conv1', **opts)\n",
    "    x = conv_layer(x, filters[1], ks, scope=block_name + '/conv2', **opts)\n",
    "    x = tf.keras.layers.MaxPooling2D(2, name=block_name + '/pool')(x)\n",
    "    return x \n",
    "\n",
    "def simple_model(input_size=[224, 224], **kwargs):\n",
    "    crop = tf.keras.Input(shape=tuple(input_size) + (3,), name='crop')\n",
    "    opts = {\n",
    "        'kernel_initializer': tf.keras.initializers.VarianceScaling(mode='fan_in', distribution='uniform'),\n",
    "        'bias_initializer': tf.keras.initializers.Constant(0.1)\n",
    "    }\n",
    "    opts = dict(opts, **kwargs)\n",
    "    \n",
    "    x = crop \n",
    "    x = conv_layer(x, 16, 3, scope='conv1', **opts)\n",
    "    x = tf.keras.layers.MaxPooling2D(3, name='pool1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1, name='dp1')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(name='bn1')(x)\n",
    "    x = conv_layer(x, 32, 3, scope='conv2', **opts)\n",
    "    x = tf.keras.layers.MaxPooling2D(3, name='pool2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1, name='dp2')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(name='bn4')(x)\n",
    "    x = conv_layer(x, 32, 3, scope='conv5', **opts)\n",
    "    x = tf.keras.layers.MaxPooling2D(2, name='pool5')(x)\n",
    "    x = tf.keras.layers.Dropout(0.25, name='dp5')(x)\n",
    "    \n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(name='bn2')(x)\n",
    "    x = conv_layer(x, 64, 3, scope='conv3', **opts)\n",
    "    x = tf.keras.layers.MaxPooling2D(3, name='pool3')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dp3')(x)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(name='bn3')(x)\n",
    "    x = conv_layer(x, 128, 3, scope='conv4', **opts)\n",
    "    x = tf.keras.layers.MaxPooling2D(3, name='pool4')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2, name='dp4')(x)\n",
    "    \n",
    "#     x = conv_block(x, filters=[16, 32], dropout_rate=0.1, scope='block1', **opts)\n",
    "#     x = conv_block(x, filters=[32, 48], dropout_rate=0.1, scope='block2', **opts)\n",
    "#     x = conv_block(x, filters=[48, 64], dropout_rate=0.1, scope='block3', **opts)\n",
    "#     x = conv_block(x, filters=[64, 128], dropout_rate=0.1, scope='block4', **opts)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='gap')(x)\n",
    "    x = tf.keras.layers.Dense(133, name='fc1')(x)\n",
    "    out = tf.keras.layers.Softmax(name='prob')(x)\n",
    "    model = tf.keras.Model(crop, out, name='simple_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 112, 112, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 112, 112, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 56, 56, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 56, 56, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 7, 7, 512)         590336    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_6 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 768,389\n",
      "Trainable params: 766,821\n",
      "Non-trainable params: 1,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=512, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 4.7220 - accuracy: 0.0355 E\n",
      "Epoch 00001: val_loss improved from inf to 5.93593, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 7s 1ms/sample - loss: 4.7205 - accuracy: 0.0358 - val_loss: 5.9359 - val_accuracy: 0.0084\n",
      "Epoch 2/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 4.1061 - accuracy: 0.0919\n",
      "Epoch 00002: val_loss improved from 5.93593 to 5.34971, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 883us/sample - loss: 4.1045 - accuracy: 0.0921 - val_loss: 5.3497 - val_accuracy: 0.0263\n",
      "Epoch 3/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 3.7029 - accuracy: 0.1534\n",
      "Epoch 00003: val_loss improved from 5.34971 to 4.14732, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 882us/sample - loss: 3.7038 - accuracy: 0.1531 - val_loss: 4.1473 - val_accuracy: 0.0826\n",
      "Epoch 4/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 3.3356 - accuracy: 0.2091\n",
      "Epoch 00004: val_loss improved from 4.14732 to 4.01320, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 884us/sample - loss: 3.3347 - accuracy: 0.2093 - val_loss: 4.0132 - val_accuracy: 0.1078\n",
      "Epoch 5/50\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 2.9908 - accuracy: 0.2736\n",
      "Epoch 00005: val_loss improved from 4.01320 to 3.45702, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 880us/sample - loss: 2.9906 - accuracy: 0.2734 - val_loss: 3.4570 - val_accuracy: 0.2024\n",
      "Epoch 6/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 2.6171 - accuracy: 0.3616\n",
      "Epoch 00006: val_loss improved from 3.45702 to 3.36810, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 882us/sample - loss: 2.6162 - accuracy: 0.3617 - val_loss: 3.3681 - val_accuracy: 0.2096\n",
      "Epoch 7/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 2.2445 - accuracy: 0.4441\n",
      "Epoch 00007: val_loss improved from 3.36810 to 3.22712, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 884us/sample - loss: 2.2476 - accuracy: 0.4433 - val_loss: 3.2271 - val_accuracy: 0.2168\n",
      "Epoch 8/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 1.9155 - accuracy: 0.5254\n",
      "Epoch 00008: val_loss did not improve from 3.22712\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 1.9142 - accuracy: 0.5254 - val_loss: 3.5293 - val_accuracy: 0.1844\n",
      "Epoch 9/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 1.5802 - accuracy: 0.6111\n",
      "Epoch 00009: val_loss improved from 3.22712 to 3.19310, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 886us/sample - loss: 1.5789 - accuracy: 0.6111 - val_loss: 3.1931 - val_accuracy: 0.2311\n",
      "Epoch 10/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 1.2738 - accuracy: 0.7021\n",
      "Epoch 00010: val_loss improved from 3.19310 to 3.10133, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 883us/sample - loss: 1.2736 - accuracy: 0.7025 - val_loss: 3.1013 - val_accuracy: 0.2551\n",
      "Epoch 11/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.9869 - accuracy: 0.7751\n",
      "Epoch 00011: val_loss improved from 3.10133 to 2.99645, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 884us/sample - loss: 0.9888 - accuracy: 0.7747 - val_loss: 2.9965 - val_accuracy: 0.2683\n",
      "Epoch 12/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.7493 - accuracy: 0.8419\n",
      "Epoch 00012: val_loss did not improve from 2.99645\n",
      "6680/6680 [==============================] - 6s 872us/sample - loss: 0.7493 - accuracy: 0.8421 - val_loss: 3.0289 - val_accuracy: 0.2719\n",
      "Epoch 13/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.5699 - accuracy: 0.8872\n",
      "Epoch 00013: val_loss did not improve from 2.99645\n",
      "6680/6680 [==============================] - 6s 876us/sample - loss: 0.5704 - accuracy: 0.8873 - val_loss: 3.0376 - val_accuracy: 0.3066\n",
      "Epoch 14/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.4161 - accuracy: 0.9265\n",
      "Epoch 00014: val_loss improved from 2.99645 to 2.95710, saving model to Data/saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 6s 883us/sample - loss: 0.4170 - accuracy: 0.9262 - val_loss: 2.9571 - val_accuracy: 0.3150\n",
      "Epoch 15/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.9517\n",
      "Epoch 00015: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 877us/sample - loss: 0.3139 - accuracy: 0.9513 - val_loss: 3.0766 - val_accuracy: 0.2886\n",
      "Epoch 16/50\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9629 ETA: 0s -\n",
      "Epoch 00016: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 873us/sample - loss: 0.2466 - accuracy: 0.9629 - val_loss: 3.4645 - val_accuracy: 0.2707\n",
      "Epoch 17/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.2010 - accuracy: 0.9731\n",
      "Epoch 00017: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.2013 - accuracy: 0.9731 - val_loss: 3.1226 - val_accuracy: 0.3198\n",
      "Epoch 18/50\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.1545 - accuracy: 0.9811\n",
      "Epoch 00018: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 0.1546 - accuracy: 0.9811 - val_loss: 3.1429 - val_accuracy: 0.2934\n",
      "Epoch 19/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1481 - accuracy: 0.9781\n",
      "Epoch 00019: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 871us/sample - loss: 0.1479 - accuracy: 0.9781 - val_loss: 3.2394 - val_accuracy: 0.2790\n",
      "Epoch 20/50\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9887\n",
      "Epoch 00020: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 0.1044 - accuracy: 0.9885 - val_loss: 3.2018 - val_accuracy: 0.3222\n",
      "Epoch 21/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1093 - accuracy: 0.9843\n",
      "Epoch 00021: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 0.1093 - accuracy: 0.9843 - val_loss: 3.2393 - val_accuracy: 0.3246\n",
      "Epoch 22/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9873\n",
      "Epoch 00022: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 876us/sample - loss: 0.1042 - accuracy: 0.9871 - val_loss: 3.2781 - val_accuracy: 0.3090\n",
      "Epoch 23/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9870\n",
      "Epoch 00023: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 0.0978 - accuracy: 0.9870 - val_loss: 3.6189 - val_accuracy: 0.2611\n",
      "Epoch 24/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9843\n",
      "Epoch 00024: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 876us/sample - loss: 0.1010 - accuracy: 0.9844 - val_loss: 3.9991 - val_accuracy: 0.2383\n",
      "Epoch 25/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9745\n",
      "Epoch 00025: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 880us/sample - loss: 0.1287 - accuracy: 0.9744 - val_loss: 3.6969 - val_accuracy: 0.2671\n",
      "Epoch 26/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1404 - accuracy: 0.9712\n",
      "Epoch 00026: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 878us/sample - loss: 0.1411 - accuracy: 0.9708 - val_loss: 4.0723 - val_accuracy: 0.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9789\n",
      "Epoch 00027: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 879us/sample - loss: 0.1168 - accuracy: 0.9789 - val_loss: 3.6795 - val_accuracy: 0.2898\n",
      "Epoch 28/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9841\n",
      "Epoch 00028: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 877us/sample - loss: 0.0900 - accuracy: 0.9841 - val_loss: 3.9657 - val_accuracy: 0.2383\n",
      "Epoch 29/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.9860\n",
      "Epoch 00029: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 877us/sample - loss: 0.0749 - accuracy: 0.9859 - val_loss: 3.8677 - val_accuracy: 0.2527\n",
      "Epoch 30/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9888\n",
      "Epoch 00030: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 881us/sample - loss: 0.0714 - accuracy: 0.9889 - val_loss: 3.7262 - val_accuracy: 0.2623\n",
      "Epoch 31/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9793\n",
      "Epoch 00031: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.0945 - accuracy: 0.9795 - val_loss: 3.7778 - val_accuracy: 0.2766\n",
      "Epoch 32/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9808\n",
      "Epoch 00032: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.0874 - accuracy: 0.9808 - val_loss: 3.9003 - val_accuracy: 0.2731\n",
      "Epoch 33/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.9834\n",
      "Epoch 00033: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 878us/sample - loss: 0.0796 - accuracy: 0.9832 - val_loss: 3.9293 - val_accuracy: 0.2443\n",
      "Epoch 34/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9835\n",
      "Epoch 00034: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 879us/sample - loss: 0.0771 - accuracy: 0.9835 - val_loss: 3.9992 - val_accuracy: 0.2611\n",
      "Epoch 35/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9826 ETA: 0s - loss: 0.0834 - accuracy: \n",
      "Epoch 00035: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 873us/sample - loss: 0.0849 - accuracy: 0.9822 - val_loss: 4.8224 - val_accuracy: 0.2096\n",
      "Epoch 36/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.9816\n",
      "Epoch 00036: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 877us/sample - loss: 0.0886 - accuracy: 0.9817 - val_loss: 3.9585 - val_accuracy: 0.2814\n",
      "Epoch 37/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9810\n",
      "Epoch 00037: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 877us/sample - loss: 0.0871 - accuracy: 0.9808 - val_loss: 4.1633 - val_accuracy: 0.2874\n",
      "Epoch 38/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9848 ETA: 0s - loss: 0.0728 \n",
      "Epoch 00038: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.0709 - accuracy: 0.9849 - val_loss: 4.0774 - val_accuracy: 0.2695\n",
      "Epoch 39/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9823\n",
      "Epoch 00039: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 877us/sample - loss: 0.0798 - accuracy: 0.9822 - val_loss: 4.5651 - val_accuracy: 0.2766\n",
      "Epoch 40/50\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9794\n",
      "Epoch 00040: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 876us/sample - loss: 0.0827 - accuracy: 0.9795 - val_loss: 4.4105 - val_accuracy: 0.2419\n",
      "Epoch 41/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9861\n",
      "Epoch 00041: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 0.0668 - accuracy: 0.9861 - val_loss: 4.1182 - val_accuracy: 0.2754\n",
      "Epoch 42/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9849\n",
      "Epoch 00042: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 880us/sample - loss: 0.0662 - accuracy: 0.9849 - val_loss: 4.2691 - val_accuracy: 0.2611\n",
      "Epoch 43/50\n",
      "6656/6680 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9821\n",
      "Epoch 00043: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 878us/sample - loss: 0.0755 - accuracy: 0.9819 - val_loss: 4.3908 - val_accuracy: 0.2323\n",
      "Epoch 44/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9828\n",
      "Epoch 00044: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.0723 - accuracy: 0.9826 - val_loss: 4.5336 - val_accuracy: 0.2635\n",
      "Epoch 45/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0586 - accuracy: 0.9863\n",
      "Epoch 00045: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 878us/sample - loss: 0.0585 - accuracy: 0.9862 - val_loss: 4.2030 - val_accuracy: 0.2766\n",
      "Epoch 46/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9861\n",
      "Epoch 00046: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 879us/sample - loss: 0.0541 - accuracy: 0.9858 - val_loss: 4.3167 - val_accuracy: 0.2395\n",
      "Epoch 47/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9857\n",
      "Epoch 00047: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 872us/sample - loss: 0.0628 - accuracy: 0.9856 - val_loss: 4.6117 - val_accuracy: 0.2647\n",
      "Epoch 48/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9843\n",
      "Epoch 00048: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 875us/sample - loss: 0.0629 - accuracy: 0.9841 - val_loss: 4.1859 - val_accuracy: 0.2659\n",
      "Epoch 49/50\n",
      "6624/6680 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9766\n",
      "Epoch 00049: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.0799 - accuracy: 0.9766 - val_loss: 4.2830 - val_accuracy: 0.2551\n",
      "Epoch 50/50\n",
      "6592/6680 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9894\n",
      "Epoch 00050: val_loss did not improve from 2.95710\n",
      "6680/6680 [==============================] - 6s 874us/sample - loss: 0.0568 - accuracy: 0.9894 - val_loss: 4.2848 - val_accuracy: 0.2778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1636971e308>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "checkpointer = ModelCheckpoint(filepath='Data/saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=2, save_best_only=True)\n",
    "\n",
    "mini.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=32, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel = simple_model(kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, **kwargs):\n",
    "    if kwargs.get('schedule'):\n",
    "        lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=kwargs.get('initial_learning_rate', 0.001),\n",
    "                                                            decay_steps=7000,\n",
    "                                                            decay_rate=0.1,\n",
    "                                                            staircase=True, name='lr_scheduler', **kwargs)\n",
    "    else: \n",
    "        lr = kwargs.get('initial_learning_rate', 0.001)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr,name='optimizer', **kwargs)\n",
    "    model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_cnn(input_size=(224,224,3), **kwargs):\n",
    "    inputs = tf.keras.Input(shape=input_size)\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_size,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "    base_model.trainable = True\n",
    "    base_model = model_with_regularization(base_model, regularizer=kwargs.get('l2'))\n",
    "    base_model.summary()\n",
    "    x = inputs \n",
    "    x = base_model(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='global_average')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='dropout1')(x)\n",
    "    x = tf.keras.layers.Dense(133, name='out')(x)\n",
    "    x = tf.keras.layers.Softmax(name='prob')(x)\n",
    "    final_model = tf.keras.Model(inputs, x, name='transfered_model')\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_regularization(model, regularizer):\n",
    "    import tempfile\n",
    "    import os\n",
    "    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):\n",
    "      print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "      return model\n",
    "\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "              setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    # Save the weights before reloading the model.\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    # load the model from the config\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pretrained_cnn(l2=tf.keras.regularizers.l2(0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, **kwargs):\n",
    "    checkpointer = ModelCheckpoint(filepath='Data/saved_models/weights.best.from_scratch.hdf5', save_best_only=True)\n",
    "    model = compile_model(model)\n",
    "    history = model.fit(train_tensors, train_targets, \n",
    "                        validation_data=(valid_tensors, valid_targets),\n",
    "                        epochs=50, batch_size=32, callbacks=[checkpointer])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(smodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    checkpointer = ModelCheckpoint(filepath='Data/saved_models/weights.best.from_scratch.hdf5', save_best_only=True)\n",
    "\n",
    "    model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=2, batch_size=32, callbacks=[checkpointer])\n",
    "    \n",
    "#     train_score = model.evaluate(train_tensors)\n",
    "#     print('train loss, train acc:', train_score)\n",
    "\n",
    "#     validation_score = model.evaluate(valid_tensors)\n",
    "#     print('validation loss, validation acc:', validation_score)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    compile_model = compile_model()\n",
    "    train_model(compile_model)\n",
    "\n",
    "# epochs = 10\n",
    "\n",
    "# ### Do NOT modify the code below this line.\n",
    "# checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "#                                verbose=2, save_best_only=True)\n",
    "\n",
    "# model.fit(train_tensors, train_targets, \n",
    "#           validation_data=(valid_tensors, valid_targets),\n",
    "#           epochs=epochs, batch_size=32, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
